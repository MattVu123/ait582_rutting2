{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17933704-3757-420a-a256-52d0714d4e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d95369bd-23b8-4990-86b5-fd4828da6d52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 583291,
     "status": "ok",
     "timestamp": 1763335770206,
     "user": {
      "displayName": "Matthew Vu",
      "userId": "04145717765483497196"
     },
     "user_tz": 300
    },
    "id": "JAgca55Lw4b_",
    "outputId": "212c3215-f688-41d0-f4f5-49833d2da986"
   },
   "outputs": [],
   "source": [
    "# ===\n",
    "# PySpark Random Forest Regressor with Hyperparameter Tuning\n",
    "# Hyperparameter tuning + 5-fold CV + Feature Importances + Testing\n",
    "# ===\n",
    "\n",
    "# Step 1: Install and import dependencies\n",
    "!pip install pyspark matplotlib\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Step 1.5: Load modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Step 2: Start Spark session\n",
    "spark = SparkSession.builder.appName(\"RandomForestTuning\").getOrCreate()\n",
    "\n",
    "# Step 3: Load dataset\n",
    "data_path = \"/content/drive/MyDrive/ait614_rutting2/data/processed/rutting_climate_traffic.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "df.printSchema()\n",
    "\n",
    "# Step 4: Define features and target\n",
    "features = [\n",
    "    \"REL_HUM_AVG_AVG\",\n",
    "    \"PRECIPITATION\",\n",
    "    \"EVAPORATION\",\n",
    "    \"PRECIP_DAYS\",\n",
    "    \"CLOUD_COVER_AVG\",\n",
    "    \"SHORTWAVE_SURFACE_AVG\",\n",
    "    \"TEMP_AVG\",\n",
    "    \"FREEZE_INDEX\",\n",
    "    \"FREEZE_THAW\",\n",
    "    \"WIND_VELOCITY_AVG\",\n",
    "    \"AADTT_VEH_CLASS_4_TREND\",\n",
    "    \"AADTT_VEH_CLASS_5_TREND\",\n",
    "    \"AADTT_VEH_CLASS_6_TREND\",\n",
    "    \"AADTT_VEH_CLASS_7_TREND\",\n",
    "    \"AADTT_VEH_CLASS_8_TREND\",\n",
    "    \"AADTT_VEH_CLASS_9_TREND\",\n",
    "    \"AADTT_VEH_CLASS_10_TREND\",\n",
    "    \"AADTT_VEH_CLASS_11_TREND\",\n",
    "    \"AADTT_VEH_CLASS_12_TREND\",\n",
    "    \"AADTT_VEH_CLASS_13_TREND\"\n",
    "]\n",
    "target = \"MAX_MEAN_DEPTH_1_8\"\n",
    "\n",
    "# Step 6: Split dataset into train/test (80/20)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"Number of targets: {len([target])}\")\n",
    "\n",
    "# Step 7: Assemble feature vector\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Step 8: Define Random Forest Regressor\n",
    "rf = RandomForestRegressor(\n",
    "    labelCol=target,\n",
    "    featuresCol=\"features\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Step 9: Build pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Step 10: Define hyperparameter grid for tuning\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [20, 100])                # default=20, try 100 for improvement\n",
    "    .addGrid(rf.maxDepth, [5, 10])                 # default=5, test deeper trees\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 4])       # default=1, stricter leaf requirement\n",
    "    .addGrid(rf.featureSubsetStrategy, [\"auto\", \"sqrt\"])  # common options\n",
    "    .build())\n",
    "\n",
    "# Step 11: Define evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Step 12: Cross-validation for hyperparameter tuning\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,       # 5-fold CV\n",
    "    parallelism=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest with 5-fold CV (this may take several minutes)...\")\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# Step 13: Get 5-fold CV RMSE for best model configuration\n",
    "best_cv_rmse = min(cv_model.avgMetrics)  # avg RMSE across folds for best hyperparameters\n",
    "print(\"\\n=== Random Forest Results ===\")\n",
    "print(f\"5-Fold CV RMSE (best hyperparameters): {best_cv_rmse:.3f}\")\n",
    "\n",
    "# Step 14: Get best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Step 15: Show best hyperparameters\n",
    "best_rf = best_model.stages[-1]\n",
    "print(\"\\n=== Best Hyperparameters ===\")\n",
    "print(f\"numTrees: {best_rf.getOrDefault('numTrees')}\")\n",
    "print(f\"maxDepth: {best_rf.getOrDefault('maxDepth')}\")\n",
    "print(f\"minInstancesPerNode: {best_rf.getOrDefault('minInstancesPerNode')}\")\n",
    "print(f\"featureSubsetStrategy: {best_rf.getOrDefault('featureSubsetStrategy')}\")\n",
    "\n",
    "# Step 16: Feature Importance Plot\n",
    "importances = best_rf.featureImportances\n",
    "feature_importances = [(features[i], importances[i]) for i in range(len(features))]\n",
    "feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "fi_df = pd.DataFrame(feature_importances, columns=[\"Feature\", \"Importance\"])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(fi_df[\"Feature\"], fi_df[\"Importance\"], color=\"steelblue\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf5fadd9-05e3-49a2-8c27-b3bd7d113b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1762809836702,
     "user": {
      "displayName": "Matthew Vu",
      "userId": "04145717765483497196"
     },
     "user_tz": 300
    },
    "id": "SwTiVlOJip90",
    "outputId": "dc2a1eae-4e40-4641-9b74-1a8ad23ecded"
   },
   "outputs": [],
   "source": [
    "# --- Evaluate best Random Forest model on the test set ---\n",
    "test_predictions = best_model.transform(test_df)\n",
    "\n",
    "# Compute test RMSE\n",
    "rmse_test = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(\"\\n=== Random Forest Test Set Evaluation ===\")\n",
    "print(f\"Test RMSE: {rmse_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83a0486a-f255-44e8-945d-13c90f52465a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yhd_CQ1olFJK"
   },
   "outputs": [],
   "source": [
    "# Step 16: Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "random_forest_notebook",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
