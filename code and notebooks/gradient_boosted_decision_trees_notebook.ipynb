{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaee799d-fa06-4a51-bcbb-0dfac51a2097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gradient-Boosted Decision Trees Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd437d31-4b27-4ef4-a360-9b43dfd9f3cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 500282,
     "status": "ok",
     "timestamp": 1763360242251,
     "user": {
      "displayName": "Matthew Vu",
      "userId": "04145717765483497196"
     },
     "user_tz": 300
    },
    "id": "Ysr8VYYbJsGW",
    "outputId": "d8a68f51-2151-4d08-e29e-52f582c87800"
   },
   "outputs": [],
   "source": [
    "# ===\n",
    "# PySpark Gradient-Boosted Trees with Hyperparameter Tuning\n",
    "# Hyperparameter tuning + 5-fold CV + Feature Importances\n",
    "# ===\n",
    "\n",
    "# Step 1: Install and import dependencies\n",
    "!pip install pyspark matplotlib\n",
    "!apt-get update -qq > /dev/null\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Step 1.5: Load modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Step 3: Start Spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"GBTTuning\").getOrCreate()\n",
    "\n",
    "# Step 4: Load dataset\n",
    "data_path = \"/content/drive/MyDrive/ait614_rutting2/data/processed/rutting_climate_traffic.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "df.printSchema()\n",
    "\n",
    "# Step 5: Define features and target\n",
    "features = [\n",
    "    \"REL_HUM_AVG_AVG\",\n",
    "    \"PRECIPITATION\",\n",
    "    \"EVAPORATION\",\n",
    "    \"PRECIP_DAYS\",\n",
    "    \"CLOUD_COVER_AVG\",\n",
    "    \"SHORTWAVE_SURFACE_AVG\",\n",
    "    \"TEMP_AVG\",\n",
    "    \"FREEZE_INDEX\",\n",
    "    \"FREEZE_THAW\",\n",
    "    \"WIND_VELOCITY_AVG\",\n",
    "    \"AADTT_VEH_CLASS_4_TREND\",\n",
    "    \"AADTT_VEH_CLASS_5_TREND\",\n",
    "    \"AADTT_VEH_CLASS_6_TREND\",\n",
    "    \"AADTT_VEH_CLASS_7_TREND\",\n",
    "    \"AADTT_VEH_CLASS_8_TREND\",\n",
    "    \"AADTT_VEH_CLASS_9_TREND\",\n",
    "    \"AADTT_VEH_CLASS_10_TREND\",\n",
    "    \"AADTT_VEH_CLASS_11_TREND\",\n",
    "    \"AADTT_VEH_CLASS_12_TREND\",\n",
    "    \"AADTT_VEH_CLASS_13_TREND\"\n",
    "]\n",
    "target = \"MAX_MEAN_DEPTH_1_8\"\n",
    "\n",
    "# Step 6: Train/test split (80/20)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"Number of targets: {len([target])}\")\n",
    "\n",
    "# Step 7: Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Step 8: Define GBT Regressor\n",
    "gbt = GBTRegressor(\n",
    "    labelCol=target,\n",
    "    featuresCol=\"features\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Step 9: Pipeline with assembler + model\n",
    "pipeline = Pipeline(stages=[assembler, gbt])\n",
    "\n",
    "# Step 10: Hyperparameter Grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(gbt.maxDepth, [3, 5])           # depth\n",
    "    .addGrid(gbt.maxIter, [20, 50])        # iterations\n",
    "    .addGrid(gbt.stepSize, [0.1, 0.2])     # learning rate\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Step 11: Evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Step 12: Cross-validation\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,\n",
    "    parallelism=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Training Gradient-Boosted Trees with 5-fold CV (this may take a while)...\")\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# Step 13: Best RMSE from CV\n",
    "best_cv_rmse = min(cv_model.avgMetrics)\n",
    "print(\"\\n=== Gradient-Boosted Trees Results ===\")\n",
    "print(f\"5-Fold CV RMSE (best hyperparameters): {best_cv_rmse:.3f}\")\n",
    "\n",
    "# Step 14: Extract best model\n",
    "best_model = cv_model.bestModel\n",
    "best_gbt = best_model.stages[-1]\n",
    "\n",
    "# Step 15: Print best hyperparameters\n",
    "print(\"\\n=== Best Hyperparameters ===\")\n",
    "print(f\"maxDepth: {best_gbt.getOrDefault('maxDepth')}\")\n",
    "print(f\"maxIter: {best_gbt.getOrDefault('maxIter')}\")\n",
    "print(f\"stepSize: {best_gbt.getOrDefault('stepSize')}\")\n",
    "\n",
    "# Step 16: Feature Importances\n",
    "importances = best_gbt.featureImportances\n",
    "feature_importances = [(features[i], importances[i]) for i in range(len(features))]\n",
    "feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "fi_df = pd.DataFrame(feature_importances, columns=[\"Feature\", \"Importance\"])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(fi_df[\"Feature\"], fi_df[\"Importance\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"GBT Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gradient_boosted_decision_trees_notebook",
   "widgets": {}
  },
  "colab": {
   "authorship_tag": "ABX9TyM60cIPYPfeo1C5br83EGan",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
