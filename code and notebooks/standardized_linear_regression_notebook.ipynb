{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23b4c9cc-eb0a-48e9-a3a0-bb67f3eeaaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Standardized Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5214f517-f46c-47d0-9810-f8af941dfa5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 154277,
     "status": "ok",
     "timestamp": 1763334136773,
     "user": {
      "displayName": "Matthew Vu",
      "userId": "04145717765483497196"
     },
     "user_tz": 300
    },
    "id": "elLt15fmYFbs",
    "outputId": "d4a40406-b3b8-42a5-f217-a6066afacb9f"
   },
   "outputs": [],
   "source": [
    "# ===\n",
    "# Linear Regression (PySpark + MLlib)\n",
    "# Handles high-VIF feature removal + hyperparameter tuning + RMSE + coefficient summary\n",
    "# ===\n",
    "\n",
    "# Step 1: Install and import dependencies\n",
    "!pip install pyspark matplotlib\n",
    "# Install Java 11\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Step 1.5: Load libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1.5: Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Step 2: Start Spark session\n",
    "spark = SparkSession.builder.appName(\"LinearRegressionTuning\").getOrCreate()\n",
    "\n",
    "# Step 3: Load dataset\n",
    "data_path = \"/content/drive/MyDrive/ait614_rutting2/data/processed/rutting_climate_traffic.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "df.printSchema()\n",
    "\n",
    "# Step 4: Define features and target\n",
    "# Removed features with VIF > 5\n",
    "removed_features = [\n",
    "    \"PRECIP_DAYS\",\n",
    "    \"REL_HUM_AVG_AVG\",\n",
    "    \"SHORTWAVE_SURFACE_AVG\",\n",
    "    \"TEMP_AVG\",\n",
    "    \"EVAPORATION\",\n",
    "    \"AADTT_VEH_CLASS_9_TREND\",\n",
    "    \"AADTT_VEH_CLASS_11_TREND\",\n",
    "    \"AADTT_VEH_CLASS_12_TREND\"\n",
    "]\n",
    "\n",
    "all_features = [\n",
    "    \"REL_HUM_AVG_AVG\",\n",
    "    \"PRECIPITATION\",\n",
    "    \"EVAPORATION\",\n",
    "    \"PRECIP_DAYS\",\n",
    "    \"CLOUD_COVER_AVG\",\n",
    "    \"SHORTWAVE_SURFACE_AVG\",\n",
    "    \"TEMP_AVG\",\n",
    "    \"FREEZE_INDEX\",\n",
    "    \"FREEZE_THAW\",\n",
    "    \"WIND_VELOCITY_AVG\",\n",
    "    \"AADTT_VEH_CLASS_4_TREND\",\n",
    "    \"AADTT_VEH_CLASS_5_TREND\",\n",
    "    \"AADTT_VEH_CLASS_6_TREND\",\n",
    "    \"AADTT_VEH_CLASS_7_TREND\",\n",
    "    \"AADTT_VEH_CLASS_8_TREND\",\n",
    "    \"AADTT_VEH_CLASS_9_TREND\",\n",
    "    \"AADTT_VEH_CLASS_10_TREND\",\n",
    "    \"AADTT_VEH_CLASS_11_TREND\",\n",
    "    \"AADTT_VEH_CLASS_12_TREND\",\n",
    "    \"AADTT_VEH_CLASS_13_TREND\"\n",
    "]\n",
    "\n",
    "# Keep only features with acceptable VIF\n",
    "features = [f for f in all_features if f not in removed_features]\n",
    "target = \"MAX_MEAN_DEPTH_1_8\"\n",
    "\n",
    "print(\"Features used (after removing high VIF):\")\n",
    "print(features)\n",
    "\n",
    "# Step 5: Split data into train/test (80/20)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"Number of targets: {len([target])}\")\n",
    "\n",
    "# Step 6: Assemble features and standardize\n",
    "# VectorAssembler combines all feature columns into a single feature vector\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features_raw\")\n",
    "# StandardScaler standardizes features to have zero mean and unit variance\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "\n",
    "# Step 7: Define Linear Regression model\n",
    "# Standardized linear regression; label column = target, features column = \"features\"\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Step 8: Create a pipeline\n",
    "# Pipeline ensures sequential execution: assemble -> scale -> standardized linear regression\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "# Step 9: Define hyperparameter grid\n",
    "# Tune regularization (ridge/lasso strength) and elasticNetParam\n",
    "# The grid tunes both regularization strength (regParam) and mixing type (elasticNetParam):\n",
    "# - regParam = 0.0 -> disables regularization entirely (no ridge, no lasso) and fits normal standardized linear regression\n",
    "# - regParam > 0.0 and elasticNetParam = 0.0 -> Ridge regression (L2)\n",
    "# - regParam > 0.0 and elasticNetParam = 1.0 -> Lasso regression (L1)\n",
    "# - regParam > 0.0 and 0 < elasticNetParam < 1 -> Elastic Net (mix of L1 & L2)\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.0, 0.01, 0.1, 0.3, 0.5])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  # 0=ridge, 1=lasso\n",
    "             .build())\n",
    "\n",
    "# Step 10: Define evaluator (RMSE)\n",
    "# RMSE is standard for regression evaluation\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Step 11: Cross-validator for 5-fold CV\n",
    "# CrossValidator performs hyperparameter tuning and returns best model\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,          # pipeline as estimator\n",
    "    estimatorParamMaps=paramGrid,# grid of hyperparameters\n",
    "    evaluator=evaluator,         # RMSE evaluator\n",
    "    numFolds=5,                  # 5-fold cross-validation\n",
    "    parallelism=2,               # parallel threads\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Training Linear Regression model with 5-fold CV...\")\n",
    "cv_model = cv.fit(train_df)  # Fit the pipeline with CV\n",
    "\n",
    "# Step 12: Get 5-fold CV RMSE for best model configuration\n",
    "best_cv_rmse = min(cv_model.avgMetrics)  # avg RMSE across folds for best hyperparameters\n",
    "print(f\"\\n=== Linear Regression Results ===\")\n",
    "print(f\"5-Fold CV RMSE (best hyperparameters): {best_cv_rmse:.3f}\")\n",
    "\n",
    "# Step 13: Print best hyperparameters\n",
    "# The best hyperparameters are in the last stage of the pipeline (the LR model)\n",
    "best_pipeline_model = cv_model.bestModel\n",
    "best_lr_model = best_pipeline_model.stages[-1]\n",
    "print(\"\\n=== Best Hyperparameters ===\")\n",
    "print(f\"Regularization Parameter (regParam): {best_lr_model._java_obj.getRegParam()}\")\n",
    "print(f\"Elastic Net Param (elasticNetParam): {best_lr_model._java_obj.getElasticNetParam()}\")\n",
    "\n",
    "# Step 14: Coefficient summary with standardized coefficients\n",
    "coefficients = best_lr_model.coefficients\n",
    "intercept = best_lr_model.intercept\n",
    "feature_names = features\n",
    "\n",
    "# Create DataFrame for easy sorting and visualization\n",
    "coef_data = {\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": coefficients,\n",
    "    \"AbsCoefficient\": np.abs(coefficients)  # absolute value for magnitude\n",
    "}\n",
    "\n",
    "coef_df = pd.DataFrame(coef_data)\n",
    "# Sort by absolute coefficient magnitude descending\n",
    "coef_df = coef_df.sort_values(by=\"AbsCoefficient\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n=== Standardized Coefficients ===\")\n",
    "print(coef_df[[\"Feature\", \"Coefficient\"]])\n",
    "\n",
    "# Step 15: Visualize standardized coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coef_df[\"Feature\"], coef_df[\"AbsCoefficient\"], color=\"steelblue\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Standardized Linear Regression Coefficient Magnitudes\")\n",
    "plt.xlabel(\"|Coefficient| (Magnitude)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 16: Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "standardized_linear_regression_notebook",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
