{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7eccb19-6605-4463-a977-b88212b5747c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebfae98e-a9e6-4583-af2a-cd55776248bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from functools import reduce\n",
    "import os\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"EDA\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87cf890d-247c-4e63-9865-947b84b2b9bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "csv_path = \"/Volumes/workspace/mlrutting-3/mlrutting-3/rutting_climate_traffic.csv\"\n",
    "\n",
    "rutting_climate_traffic = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(csv_path)\n",
    "\n",
    "# View schema and first rows\n",
    "rutting_climate_traffic.printSchema()\n",
    "rutting_climate_traffic.show(5)\n",
    "\n",
    "# Print row count\n",
    "print(\"Number of rows:\", rutting_climate_traffic.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f4a231-e1b9-4983-a52f-37d34de82249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split into training (80%) and test (20%)\n",
    "train_df, test_df = rutting_climate_traffic.randomSplit([0.8, 0.2], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "324444ae-1b9a-4a91-a25a-eec78fd0d43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# declare features\n",
    "features = [\n",
    "    \"REL_HUM_AVG_AVG\",\n",
    "    \"PRECIPITATION\",\n",
    "    \"EVAPORATION\",\n",
    "    \"PRECIP_DAYS\",\n",
    "    \"CLOUD_COVER_AVG\",\n",
    "    \"SHORTWAVE_SURFACE_AVG\",\n",
    "    \"TEMP_AVG\",\n",
    "    \"FREEZE_INDEX\",\n",
    "    \"FREEZE_THAW\",\n",
    "    \"WIND_VELOCITY_AVG\",\n",
    "    \"AADTT_VEH_CLASS_4_TREND\",\n",
    "    \"AADTT_VEH_CLASS_5_TREND\",\n",
    "    \"AADTT_VEH_CLASS_6_TREND\",\n",
    "    \"AADTT_VEH_CLASS_7_TREND\",\n",
    "    \"AADTT_VEH_CLASS_8_TREND\",\n",
    "    \"AADTT_VEH_CLASS_9_TREND\",\n",
    "    \"AADTT_VEH_CLASS_10_TREND\",\n",
    "    \"AADTT_VEH_CLASS_11_TREND\",\n",
    "    \"AADTT_VEH_CLASS_12_TREND\",\n",
    "    \"AADTT_VEH_CLASS_13_TREND\"\n",
    "]\n",
    "\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "\n",
    "feature_df = train_df.select(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44fd611-7fda-43c9-a486-27b185d953ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas\n",
    "feature_pd = feature_df.toPandas()\n",
    "\n",
    "# Compute VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = feature_pd.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(feature_pd.values, i) \n",
    "                   for i in range(feature_pd.shape[1])]\n",
    "\n",
    "print(vif_data.sort_values(by=\"VIF\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea69fa4-d1b9-410e-ab3e-a91f8117d8fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Include the target variable along with features\n",
    "corr_df = train_df.select(features + [\"MAX_MEAN_DEPTH_1_8\"]).toPandas()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = corr_df.corr()\n",
    "\n",
    "# Display correlation matrix\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Correlation Matrix Including Rutting Depth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf7899cf-3157-4b32-86d2-2b5ce59ce003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Convert full DataFrame to Pandas\n",
    "plot_vars = features + [\"MAX_MEAN_DEPTH_1_8\"]\n",
    "boxplot_df = train_df.select(plot_vars).toPandas()\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "for col in plot_vars:\n",
    "    boxplot_df[col] = pd.to_numeric(boxplot_df[col], errors='coerce')\n",
    "\n",
    "# Define grid layout\n",
    "n_features = len(plot_vars)\n",
    "n_cols = 4\n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, n_rows * 3))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each feature separately\n",
    "for i, var in enumerate(plot_vars):\n",
    "    sns.boxplot(\n",
    "        y=boxplot_df[var],\n",
    "        ax=axes[i],\n",
    "        color=\"skyblue\",\n",
    "        fliersize=1\n",
    "    )\n",
    "    axes[i].set_title(var, fontsize=10)  # FIXED\n",
    "    axes[i].set_ylabel(\"Value\")\n",
    "\n",
    "# Hide any unused subplots\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Adjust layout\n",
    "fig.suptitle(\"Boxplots of Variable Distributions\", fontsize=14, y=0.995)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79effce-1800-4b81-bc5c-2dc64a3cc1dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas\n",
    "cols = features + [\"MAX_MEAN_DEPTH_1_8\"]\n",
    "df_pd = train_df.select(cols).toPandas()\n",
    "\n",
    "# Make sure all columns are numeric\n",
    "for col in cols:\n",
    "    df_pd[col] = pd.to_numeric(df_pd[col], errors='coerce')\n",
    "\n",
    "# Melt data for facet plotting\n",
    "melted = df_pd.melt(\n",
    "    id_vars=[\"MAX_MEAN_DEPTH_1_8\"],\n",
    "    value_vars=features,\n",
    "    var_name=\"Feature\",\n",
    "    value_name=\"Value\"\n",
    ")\n",
    "\n",
    "# Create FacetGrid with 3 columns\n",
    "g = sns.FacetGrid(\n",
    "    melted,\n",
    "    col=\"Feature\",\n",
    "    col_wrap=4,\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    height=3.5\n",
    ")\n",
    "g.map(sns.scatterplot, \"Value\", \"MAX_MEAN_DEPTH_1_8\", alpha=0.5)\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.set_axis_labels(\"Feature Value\", \"Rutting Depth (mm)\")\n",
    "plt.subplots_adjust(top=0.95, hspace=0.5)\n",
    "g.fig.suptitle(\"Scatterplots of Features vs. Rutting Depth\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f219686-b7cf-4344-90f8-ad8f556717ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "cols = features + [\"MAX_MEAN_DEPTH_1_8\"]\n",
    "df = train_df.select(cols)\n",
    "\n",
    "# Initialize a list to store summary stats\n",
    "stats = []\n",
    "\n",
    "# Loop through columns and compute summary stats\n",
    "for col in cols:\n",
    "    # Compute approximate quantiles for 25th, 50th, 75th percentiles\n",
    "    q25, q50, q75 = df.approxQuantile(col, [0.25, 0.5, 0.75], 0.01)\n",
    "\n",
    "    # Compute min and max using Spark built-ins\n",
    "    agg = df.select(\n",
    "        F.min(F.col(col).cast(\"double\")).alias(\"min\"),\n",
    "        F.max(F.col(col).cast(\"double\")).alias(\"max\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    # Calculate IQR\n",
    "    iqr = q75 - q25\n",
    "\n",
    "    # Store results (ensure all numeric values are float)\n",
    "    stats.append({\n",
    "        \"Feature\": col,\n",
    "        \"Median\": float(round(q50, 3)),\n",
    "        \"IQR\": float(round(iqr, 3)),\n",
    "        \"Min\": float(round(agg[\"min\"], 3)),\n",
    "        \"Max\": float(round(agg[\"max\"], 3))\n",
    "    })\n",
    "\n",
    "# Convert list to Spark DataFrame safely\n",
    "stats_df = spark.createDataFrame(stats)\n",
    "\n",
    "# Display neatly\n",
    "stats_df.show(len(cols), truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EDA_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
