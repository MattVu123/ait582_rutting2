{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eef22891-53e1-46e6-82e6-bd2da5db8a5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Average Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf980359-b80d-493b-b0b7-5136c22f7adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91322,
     "status": "ok",
     "timestamp": 1763334066084,
     "user": {
      "displayName": "Matthew Vu",
      "userId": "04145717765483497196"
     },
     "user_tz": 300
    },
    "id": "XABi523tUfuD",
    "outputId": "07a58b10-afd9-4514-af02-049db64564fb"
   },
   "outputs": [],
   "source": [
    "# ===\n",
    "#   BASELINE MODEL (PySpark + MLlib)\n",
    "#   Predicts mean target value from training set\n",
    "#   Evaluates with RMSE using 5-fold CV and test set\n",
    "# ===\n",
    "\n",
    "# Step 1: Install and import PySpark\n",
    "!pip install pyspark matplotlib\n",
    "!apt-get update -qq > /dev/null\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Step 1.5: Load libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, lit\n",
    "import numpy as np\n",
    "\n",
    "# Step 2: Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Step 3: Start Spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"BaselineModel\").getOrCreate()\n",
    "\n",
    "# Step 4: Define file path\n",
    "# Make sure the file is in *your own Drive*\n",
    "data_path = \"/content/drive/MyDrive/ait614_rutting2/data/processed/rutting_climate_traffic.csv\"\n",
    "\n",
    "# Step 5: Load dataset\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "df.printSchema()\n",
    "\n",
    "# Step 6: Split dataset into train/test (80/20)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(\"\\nData Split:\")\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")\n",
    "\n",
    "# Step 7: Define target variable\n",
    "target = \"MAX_MEAN_DEPTH_1_8\"\n",
    "\n",
    "# Step 8: Compute mean target from training set (baseline prediction)\n",
    "train_mean = train_df.select(F.mean(col(target))).collect()[0][0]\n",
    "print(f\"\\nAverage target value (baseline prediction): {train_mean:.3f}\")\n",
    "\n",
    "# Step 9: Add prediction column to test set\n",
    "test_pred_df = test_df.withColumn(\"prediction\", lit(train_mean))\n",
    "\n",
    "# Step 10: Evaluate RMSE on test set\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rmse_test = evaluator.evaluate(test_pred_df)\n",
    "print(\"\\n=== Baseline Model Results ===\")\n",
    "print(f\"Average target value (baseline prediction): {train_mean:.3f}\")\n",
    "print(f\"Test RMSE: {rmse_test:.3f}\")\n",
    "\n",
    "# Step 11: Compute 5-fold CV RMSE on training set\n",
    "# Trick: Use LinearRegression with zero features to mimic constant prediction\n",
    "assembler = VectorAssembler(inputCols=[], outputCol=\"features\")  # empty feature vector\n",
    "lr_dummy = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    fitIntercept=True\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, lr_dummy])\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=[{}],  # no hyperparameters to tune\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,\n",
    "    parallelism=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(train_df)\n",
    "best_cv_rmse = min(cv_model.avgMetrics)  # only one configuration\n",
    "print(f\"Average 5-Fold CV RMSE (baseline): {best_cv_rmse:.3f}\")\n",
    "\n",
    "# Step 12: Stop Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "baseline_notebook",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
