{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c50ecfe0-1192-4d48-baf3-623307f338b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c99ecb9-5a35-44d7-9979-3aa898858648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 599321,
     "status": "ok",
     "timestamp": 1763335784837,
     "user": {
      "displayName": "Matthew Vu",
      "userId": "04145717765483497196"
     },
     "user_tz": 300
    },
    "id": "mqQjMhSOYF6y",
    "outputId": "e94aca53-1b5e-49ca-cbeb-36ca005ddcf4"
   },
   "outputs": [],
   "source": [
    "# ===\n",
    "# Decision Tree Regressor (PySpark + MLlib)\n",
    "# Hyperparameter tuning + 5-fold CV + Feature Importances\n",
    "# ===\n",
    "\n",
    "# Step 1: Install dependencies\n",
    "!pip install pyspark matplotlib\n",
    "!apt-get update -qq > /dev/null\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Step 1.5: load libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- Step 2.5: Mount Google Drive ---\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# --- Step 2: Start Spark session ---\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"DecisionTreeTuning\").getOrCreate()\n",
    "\n",
    "# --- Step 3: Load dataset ---\n",
    "data_path = \"/content/drive/MyDrive/ait614_rutting2/data/processed/rutting_climate_traffic.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "df.printSchema()\n",
    "\n",
    "# --- Step 4: Define features and target ---\n",
    "features = [\n",
    "    \"REL_HUM_AVG_AVG\",\n",
    "    \"PRECIPITATION\",\n",
    "    \"EVAPORATION\",\n",
    "    \"PRECIP_DAYS\",\n",
    "    \"CLOUD_COVER_AVG\",\n",
    "    \"SHORTWAVE_SURFACE_AVG\",\n",
    "    \"TEMP_AVG\",\n",
    "    \"FREEZE_INDEX\",\n",
    "    \"FREEZE_THAW\",\n",
    "    \"WIND_VELOCITY_AVG\",\n",
    "    \"AADTT_VEH_CLASS_4_TREND\",\n",
    "    \"AADTT_VEH_CLASS_5_TREND\",\n",
    "    \"AADTT_VEH_CLASS_6_TREND\",\n",
    "    \"AADTT_VEH_CLASS_7_TREND\",\n",
    "    \"AADTT_VEH_CLASS_8_TREND\",\n",
    "    \"AADTT_VEH_CLASS_9_TREND\",\n",
    "    \"AADTT_VEH_CLASS_10_TREND\",\n",
    "    \"AADTT_VEH_CLASS_11_TREND\",\n",
    "    \"AADTT_VEH_CLASS_12_TREND\",\n",
    "    \"AADTT_VEH_CLASS_13_TREND\"\n",
    "]\n",
    "target = \"MAX_MEAN_DEPTH_1_8\"\n",
    "\n",
    "# --- Step 5: Split dataset into train/test (80/20) ---\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training set size: {train_df.count()}\")\n",
    "print(f\"Test set size: {test_df.count()}\")\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "print(f\"Number of targets: {len([target])}\")\n",
    "\n",
    "# Step 6: Assemble feature vector\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "\n",
    "# Step 7: Define Decision Tree Regressor\n",
    "dt = DecisionTreeRegressor(\n",
    "    labelCol=target,\n",
    "    featuresCol=\"features\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Step 8: Build pipeline\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n",
    "\n",
    "# Step 9: Define hyperparameter grid\n",
    "# Criterion hyperparameter -> only \"variance\" is supported for regression\n",
    "paramGrid = (ParamGridBuilder()\n",
    "    .addGrid(dt.maxDepth, [5, 10, 15])\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 2, 4])   # similar to min_samples_leaf\n",
    "    .addGrid(dt.minInfoGain, [0.0, 0.001, 0.01, 0.05]) # minimum info gain required for split\n",
    "    .addGrid(dt.maxBins, [32, 64])                # affects feature splits\n",
    "    .build())\n",
    "\n",
    "# Step 10: Define evaluator (RMSE)\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=target,\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "# Step 11: Cross-validation for hyperparameter tuning\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,   # 5-fold CV\n",
    "    parallelism=2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Training Decision Tree with 5-fold CV (this may take several minutes)...\")\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# Step 12: Get 5-fold CV RMSE for best model config\n",
    "best_cv_rmse = min(cv_model.avgMetrics)  # avg RMSE across folds for best hyperparameters\n",
    "print(\"\\n=== Decision Tree Results ===\")\n",
    "print(f\"5-Fold CV RMSE (best hyperparameters): {best_cv_rmse:.3f}\")\n",
    "\n",
    "# Step 13: Get best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Step 14: Show best hyperparameters\n",
    "best_dt = best_model.stages[-1]\n",
    "print(\"\\n=== Best Hyperparameters ===\")\n",
    "print(f\"Max Depth: {best_dt.getOrDefault('maxDepth')}\")\n",
    "print(f\"Min Instances per Node (min_samples_leaf): {best_dt.getOrDefault('minInstancesPerNode')}\")\n",
    "print(f\"Min Info Gain: {best_dt.getOrDefault('minInfoGain')}\")\n",
    "print(f\"Max Bins: {best_dt.getOrDefault('maxBins')}\")\n",
    "print(f\"Criterion: variance (default and only supported for regression)\")\n",
    "\n",
    "# Step 15: Feature Importance Plot\n",
    "importances = best_dt.featureImportances\n",
    "feature_importances = [(features[i], importances[i]) for i in range(len(features))]\n",
    "feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "fi_df = pd.DataFrame(feature_importances, columns=[\"Feature\", \"Importance\"])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(fi_df[\"Feature\"], fi_df[\"Importance\"], color=\"steelblue\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Decision Tree Feature Importances\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 16: Stop Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "decision_tree_notebook",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
